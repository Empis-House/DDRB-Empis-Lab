# DDRB-Empis-Lab


Principal Objective:
Create a model which accelerates and diversifies the learning process of classic Mario Bros gameplay ML models via Procedural Content Generation ML and a variant of Dynamic Difficulty Adjustment where we propose that it is necessarily considered a reward-difficulty balancing stead of a direct Difficulty Adjustment.

Sub Objectives:

    1. Develop a PCGML model which considers a latent representation of a Level and the User/Agent Jump performance description.
    
    2. Generate Map Elite Grammar representations of a generative basis of preselected feasible sub-levels which represent a great variety in a fixed expected jump performance space.
    
    3. Implement an RL Agent to balance the Reward component, setting all rewards* across the level to induce the User/Agent to achieve the expected jump performance.
    
    4. Create an Environment Manager which controls hyperparameters of previous components to optimize long-term User/Agent results with increasing Objective performance.

    5. Test the complete environment with different Mario Bros player ML-agents. The model's objective is to improve the original ML-agent's robustness. Additionally, we believe our model would accelerate its convergence to optimal performance.
    
* Also, we expect to test it in a real game, with a version of the Dynamic Difficulty Reward Balancing trained with multiple Artificial Agents.

Notes: Consider a stochastic definition of Reward where we can represent, for example, an enemy which is a potential negative reward if it kills you, but also a potential positive reward if you defeat it before.

**DOCUMENT IN PROGRESS**
